{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22a38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3de50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_set = datasets.MNIST('./data', train=True, download=True)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "train_set = train_set.data.numpy()\n",
    "test_set = test_set.data.numpy()\n",
    "\n",
    "train_set = train_set / 255.0\n",
    "test_set = test_set / 255.0\n",
    "\n",
    "train_set = train_set.reshape(-1, 784)\n",
    "test_set = test_set.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fcc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(image):\n",
    "    plt.imshow(image, cmap='gray') \n",
    "    plt.axis('off') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02bf66a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = int(input_size / 2)\n",
    "latent_size = 32\n",
    "preview(test_set[0].reshape(28, 28))\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fdb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) / (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c00b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c5a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/przxnqys7mv69gvj61446b2w0000gn/T/ipykernel_91151/4100074866.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.026114743160118734\n",
      "Epoch 1, Loss: 0.026115217054040803\n",
      "Epoch 2, Loss: 0.026115238843475372\n",
      "Epoch 3, Loss: 0.026115234940500345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m train_set:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         z1, a1, z2, a2, z3, a3, z4, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(y \u001b[38;5;241m-\u001b[39m y_pred, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m# Mean Square Loss function\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         dz4 \u001b[38;5;241m=\u001b[39m y_pred \u001b[38;5;241m-\u001b[39m y  \u001b[38;5;66;03m# Assuming a4 is the output of your network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     13\u001b[0m z1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(y, W1) \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# input_size -> hidden_size\u001b[39;00m\n\u001b[1;32m     14\u001b[0m a1 \u001b[38;5;241m=\u001b[39m sigmoid(z1)\n\u001b[0;32m---> 15\u001b[0m z2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b2 \u001b[38;5;66;03m# hidden_size -> latent_size\u001b[39;00m\n\u001b[1;32m     16\u001b[0m a2 \u001b[38;5;241m=\u001b[39m relu(z2)\n\u001b[1;32m     17\u001b[0m z3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a2, W3) \u001b[38;5;241m+\u001b[39m b3 \u001b[38;5;66;03m# latent_size -> hidden_size\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# y -> z1 -> z2 -> z3 -> y_pred\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights for the hidden layer\n",
    "b1 = np.zeros((1, hidden_size))  # Biases for the hidden layer\n",
    "W2 = np.random.randn(hidden_size, latent_size)  # Weights for the output layer\n",
    "b2 = np.zeros((1, latent_size))\n",
    "W3 = np.random.randn(latent_size, hidden_size)  # Weights for the hidden layer\n",
    "b3 = np.zeros((1, hidden_size))  # Biases for the hidden layer\n",
    "W4 = np.random.randn(hidden_size, input_size)  # Weights for the output layer\n",
    "b4 = np.zeros((1, input_size))\n",
    "\n",
    "def forward(y):\n",
    "    z1 = np.dot(y, W1) + b1 # input_size -> hidden_size\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2 # hidden_size -> latent_size\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(a2, W3) + b3 # latent_size -> hidden_size\n",
    "    a3 = sigmoid(z3)  \n",
    "    z4 = a3.dot(W4) + b4 # hidden_size -> input_size\n",
    "    a4 = relu(z4)\n",
    "    return z1, a1, z2, a2, z3, a3, z4, a4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for y in train_set:\n",
    "        # Forward propagation\n",
    "        z1, a1, z2, a2, z3, a3, z4, y_pred = forward(y)\n",
    "    \n",
    "        loss = np.mean(0.5 * np.power(y - y_pred, 2)) # Mean Square Loss function\n",
    "        \n",
    "        dz4 = y_pred - y  # Assuming a4 is the output of your network\n",
    "        dW4 = a3.T.dot(dz4)  # Gradient of loss w.r.t. W4\n",
    "        db4 = np.sum(dz4, axis=0)  # Gradient of loss w.r.t. b4\n",
    "            \n",
    "        dz3 = dz4.dot(W4.T) * a3 * (1 - a3)  # Gradient of loss w.r.t. z3 using sigmoid derivative\n",
    "        dW3 = a2.T.dot(dz3)  # Gradient of loss w.r.t. W3\n",
    "        db3 = np.sum(dz3, axis=0)  # Gradient of loss w.r.t. b3\n",
    "        \n",
    "        dz2 = dz3.dot(W3.T) * (z2 > 0)  # Gradient of loss w.r.t. z2 using ReLU derivative\n",
    "        dW2 = a1.T.dot(dz2)  # Gradient of loss w.r.t. W2\n",
    "        db2 = np.sum(dz2, axis=0)  # Gradient of loss w.r.t. b2\n",
    "        \n",
    "        dz1 = dz2.dot(W2.T) * a1 * (1 - a1)  # Gradient of loss w.r.t. z1 using sigmoid derivative\n",
    "        dW1 = W1.dot(dz1.T)  # Gradient of loss w.r.t. W1\n",
    "        db1 = np.sum(dz1, axis=0)  # Gradient of loss w.r.t. b1\n",
    "        \n",
    "        # Update weights and biases for each layer\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "        \n",
    "        W2 = W2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        \n",
    "        W3 = W3 - learning_rate * dW3\n",
    "        b3 = b3 - learning_rate * db3\n",
    "        \n",
    "        W4 = W4 - learning_rate * dW4\n",
    "        b4 = b4 - learning_rate * db4\n",
    "        \n",
    "    print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5262f766-63c5-4059-8b13-81b8c9698f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m train_set:\n\u001b[0;32m---> 87\u001b[0m     _, _, _, _, _, _, _, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(y \u001b[38;5;241m-\u001b[39m y_pred, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     89\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(y):\n\u001b[0;32m---> 18\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# input_size -> hidden_size\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m sigmoid(z1)\n\u001b[1;32m     20\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a1, W2) \u001b[38;5;241m+\u001b[39m b2 \u001b[38;5;66;03m# hidden_size -> latent_size\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the batch size (number of samples in each mini-batch)\n",
    "batch_size = 32  # You can adjust this value based on your dataset and available memory\n",
    "\n",
    "# Split your training data into mini-batches\n",
    "num_samples = len(train_set)\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights for the hidden layer\n",
    "b1 = np.zeros((1, hidden_size))  # Biases for the hidden layer\n",
    "W2 = np.random.randn(hidden_size, latent_size)  # Weights for the output layer\n",
    "b2 = np.zeros((1, latent_size))\n",
    "W3 = np.random.randn(latent_size, hidden_size)  # Weights for the hidden layer\n",
    "b3 = np.zeros((1, hidden_size))  # Biases for the hidden layer\n",
    "W4 = np.random.randn(hidden_size, input_size)  # Weights for the output layer\n",
    "b4 = np.zeros((1, input_size))\n",
    "\n",
    "def forward(y):\n",
    "    z1 = np.dot(y, W1) + b1 # input_size -> hidden_size\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2 # hidden_size -> latent_size\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(a2, W3) + b3 # latent_size -> hidden_size\n",
    "    a3 = sigmoid(z3)  \n",
    "    z4 = a3.dot(W4) + b4 # hidden_size -> input_size\n",
    "    a4 = relu(z4)\n",
    "    return z1, a1, z2, a2, z3, a3, z4, a4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle your training data at the beginning of each epoch to randomize mini-batch order\n",
    "    np.random.shuffle(train_set)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Extract a mini-batch\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        mini_batch = train_set[start:end]\n",
    "        \n",
    "        # Initialize gradients for each batch\n",
    "        dW1_batch, db1_batch = 0, 0\n",
    "        dW2_batch, db2_batch = 0, 0\n",
    "        dW3_batch, db3_batch = 0, 0\n",
    "        dW4_batch, db4_batch = 0, 0\n",
    "        \n",
    "        # Process each sample in the mini-batch\n",
    "        for y in mini_batch:\n",
    "            # Forward propagation\n",
    "            z1, a1, z2, a2, z3, a3, z4, y_pred = forward(y)\n",
    "\n",
    "            dz4 = y_pred - y  # Assuming a4 is the output of your network\n",
    "            dW4 = a3.T.dot(dz4)  # Gradient of loss w.r.t. W4\n",
    "            db4 = np.sum(dz4, axis=0)  # Gradient of loss w.r.t. b4\n",
    "                \n",
    "            dz3 = dz4.dot(W4.T) * a3 * (1 - a3)  # Gradient of loss w.r.t. z3 using sigmoid derivative\n",
    "            dW3 = a2.T.dot(dz3)  # Gradient of loss w.r.t. W3\n",
    "            db3 = np.sum(dz3, axis=0)  # Gradient of loss w.r.t. b3\n",
    "            \n",
    "            dz2 = dz3.dot(W3.T) * (z2 > 0)  # Gradient of loss w.r.t. z2 using ReLU derivative\n",
    "            dW2 = a1.T.dot(dz2)  # Gradient of loss w.r.t. W2\n",
    "            db2 = np.sum(dz2, axis=0)  # Gradient of loss w.r.t. b2\n",
    "            \n",
    "            dz1 = dz2.dot(W2.T) * a1 * (1 - a1)  # Gradient of loss w.r.t. z1 using sigmoid derivative\n",
    "            dW1 = W1.dot(dz1.T)  # Gradient of loss w.r.t. W1\n",
    "            db1 = np.sum(dz1, axis=0)  # Gradient of loss w.r.t. b1\n",
    "\n",
    "            dW1_batch += dW1\n",
    "            dW2_batch += dW2\n",
    "            dW3_batch += dW3\n",
    "            dW4_batch += dW4\n",
    "\n",
    "        \n",
    "        W1 = W1 - (learning_rate / batch_size) * (dW1_batch / batch_size)\n",
    "        b1 = b1 - (learning_rate / batch_size) * (db1_batch / batch_size)\n",
    "\n",
    "        W2 = W2 - (learning_rate / batch_size) * (dW2_batch / batch_size)\n",
    "        b2 = b2 - (learning_rate / batch_size) * (db2_batch / batch_size)\n",
    "\n",
    "        W3 = W3 - (learning_rate / batch_size) * (dW3_batch / batch_size)\n",
    "        b3 = b3 - (learning_rate / batch_size) * (db3_batch / batch_size)\n",
    "        \n",
    "        W4 = W4 - (learning_rate / batch_size) * (dW4_batch / batch_size)\n",
    "        b4 = b4 - (learning_rate / batch_size) * (db4_batch / batch_size)\n",
    "        \n",
    "        \n",
    "    # Calculate and print the loss at the end of each epoch\n",
    "    total_loss = 0\n",
    "    for y in train_set:\n",
    "        _, _, _, _, _, _, _, y_pred = forward(y)\n",
    "        loss = np.mean(0.5 * np.power(y - y_pred, 2))\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / num_samples\n",
    "    print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "421afd9e-22a1-45ff-a417-0fa6245ab70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHH0lEQVR4nO3cMY+MaxiA4ZljNXQSsh2VkKwgUUhINCrRaFV+gMT/0PoJKskmChFaSg2lqBQqiq1WNHynce7qnJx555yZHeu66u/J+1Zz5ynmnU/TNM0AYDab/XHQFwBgc4gCABEFACIKAEQUAIgoABBRACCiAEC2Fv1wPp+v8h4ArNgi/1W2KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAbB30BeB3c/bs2aXm3r9/Pzzz4MGD4ZlHjx4Nz3B42BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEA8iAdrdvny5aXmfvz4MTzz6dOnpc7i92VTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8SAerNmlS5eWmtvf3x+eefr06VJn8fuyKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgHgQD/6DnZ2d4Zn79+8vddbjx4+XmoMRNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACBeSYX/4Ny5c8Mzx48fX+qsJ0+eLDUHI2wKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg82mapoU+nM9XfRf45bx582Z45uTJk0udtbOzMzyzv7+/1FkcTov83NsUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAtg76ArApzpw5Mzxz5cqV4ZkPHz4Mz8xmHrdjPWwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgHsSDn27cuLGWc758+bKWc2AZNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACBeSYWfLly4sJZzHj58uJZzYBk2BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkPk0TdNCH87nq74L/G+uXr06PPP8+fPhmY8fPw7PXLt2bXhmNpvNvn37ttQc/GWRn3ubAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyNZBXwBW4ebNm8MzJ06cGJ55+fLl8IyH7dhkNgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAP4nEoXbx4cXhmmqbhmd3d3eEZ2GQ2BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkPm04Ctg8/l81XeBv7W9vT088+7du+GZvb294Znz588Pz8BBWeTn3qYAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBk66AvAP/m3r17wzOnTp0annnx4sXwDBw2NgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAP4rHxTp8+vZZz9vb21nIObDKbAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiAfx2Hi3b99eyznPnj1byzmwyWwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgHsRjba5fv77U3Pb29v98E+Cf2BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEA8iMfa3LlzZ6m5I0eODM+8fft2eOb169fDM3DY2BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYB4JZWlHDt2bHjm1q1bK7jJ39vd3R2e+f79+wpuAr8WmwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMh8mqZpoQ/n81XfhV/I0aNHh2devXq11FmfP38enrl79+7wzNevX4dn4FeyyM+9TQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAMSDeAC/CQ/iATBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgW4t+OE3TKu8BwAawKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkD8BH5aUBGTeu5MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = test_set[2]\n",
    "preview(sample.reshape(28, 28))\n",
    "_, _, _, _, _, _, _, result = forward(sample)\n",
    "preview(result.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52696847-e6ff-422f-b646-8037bd8abd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
